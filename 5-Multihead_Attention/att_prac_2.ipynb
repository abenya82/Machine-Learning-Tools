{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q: [[-0.64160862  0.30220004 -0.61595845  1.20250417 -0.18510138  0.33408615\n",
      "  -0.66467492 -1.56618377]\n",
      " [-0.05870135  1.47366876  0.47503371 -1.29730791 -1.73193787 -0.95141771\n",
      "   0.68898643 -0.18032509]\n",
      " [-0.1328231   0.2566022  -1.35322708 -0.56658146  0.26455423 -0.81648327\n",
      "  -0.51199825  0.3024339 ]\n",
      " [ 0.03559231 -0.04839526  1.56351574  0.4234759   0.52231569 -0.66089308\n",
      "   0.61332584  0.90518989]] \n",
      " (4, 8)\n"
     ]
    }
   ],
   "source": [
    "L, d_k,d_v = 4,8,8\n",
    "\n",
    "q = np.random.randn(L,d_k)\n",
    "k = np.random.randn(L,d_k)\n",
    "v = np.random.randn(L,d_v)\n",
    "\n",
    "print(f'q: {q} \\n {q.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Self Attention\n",
    "\n",
    "$self attention = softmax{({QK^T \\over \\sqrt{d_k}} +M)V}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.23973136,  2.42082008,  2.6021219 , -1.24145812],\n",
       "       [ 4.47763001,  1.53169749, -1.26457367, -2.29504025],\n",
       "       [ 1.6052039 ,  1.61075147, -1.91492468, -0.65375085],\n",
       "       [-2.40674416, -0.73653602, -1.69170908,  3.42647261]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.matmul(q,k.T)\n",
    "# same as:\n",
    "q@k.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.43831122,  0.85588915,  0.91998902, -0.43892173],\n",
       "       [ 1.58308127,  0.54153684, -0.44709431, -0.81141926],\n",
       "       [ 0.56752528,  0.56948664, -0.67702811, -0.23113583],\n",
       "       [-0.85091256, -0.26040481, -0.59810948,  1.21144101]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled_q_k = q@k.T / np.sqrt(d_k)\n",
    "scaled_q_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.6721539920638319, 1.2456821566762712, 0.57856078371441)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q.var(),k.var(),scaled_q_k.var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0.],\n",
       "       [1., 1., 0., 0.],\n",
       "       [1., 1., 1., 0.],\n",
       "       [1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#masking\n",
    "# lets words only look at tokens that came before it\n",
    "mask = np.tril(np.ones((L,L)))\n",
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0., -inf, -inf, -inf],\n",
       "       [  0.,   0., -inf, -inf],\n",
       "       [  0.,   0.,   0., -inf],\n",
       "       [  0.,   0.,   0.,   0.]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask[mask==0] = -np.infty\n",
    "mask[mask==1] = 0\n",
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.43831122,        -inf,        -inf,        -inf],\n",
       "       [ 1.58308127,  0.54153684,        -inf,        -inf],\n",
       "       [ 0.56752528,  0.56948664, -0.67702811,        -inf],\n",
       "       [-0.85091256, -0.26040481, -0.59810948,  1.21144101]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled_q_k + mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#softmax converts vector into prob distribution\n",
    "def softmax(x):\n",
    "    return (np.exp(x).T/np.sum(np.exp(x),axis=-1)).T\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.        , 0.        , 0.        ],\n",
       "       [0.7391479 , 0.2608521 , 0.        , 0.        ],\n",
       "       [0.43667496, 0.43753228, 0.12579275, 0.        ],\n",
       "       [0.08363307, 0.15094972, 0.10768843, 0.65772877]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention = softmax(scaled_q_k + mask)\n",
    "attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.4725925 ,  0.81298057, -1.43956008,  0.21747511,  2.15076971,\n",
       "        -0.05137905,  1.10195603, -1.07178402],\n",
       "       [-0.09684792,  0.87059508, -0.96400721, -0.3346464 ,  1.52854287,\n",
       "        -0.04017561,  0.7505509 , -1.23124724],\n",
       "       [ 0.16778739,  0.79517111, -0.25758252, -0.61963966,  0.78613471,\n",
       "        -0.0046214 ,  0.20529551, -1.02307423],\n",
       "       [ 0.32832328,  0.28124412,  0.7735031 , -0.31600945,  0.55560734,\n",
       "         0.996375  , -0.44048683,  0.05990275]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_v = attention @ v\n",
    "new_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When $a \\ne 0$, there are two solutions to $(ax^2 + bx + c = 0)$ and they are \n",
    "$$ x = {-b \\pm \\sqrt{b^2-4ac} \\over 2a} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NN_prac",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
